{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "interpreter": {
      "hash": "82f7237e8744fa450af06f16fd99e4f987c4ded027c30bb82e91e6fd7991cebc"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.10 64-bit ('base': conda)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sbowma15/Machine_Learning_and_Data_Training/blob/main/Machine_Learning_and_Data_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNl__FziQe4O"
      },
      "source": [
        "# CS 39AA - Traditional ML\n",
        "\n",
        "\n",
        "Everything that we will look at in this course is a form of supervised learning (even though some of the NLP models we will look at it don't appear to be at first glance).\n",
        "\n",
        "As we already discussed, supervised learning can roughly be split into two types of tasks, either regression or classification. We'll look at these each in turn now.\n",
        "\n",
        "## I. Regression\n",
        "\n",
        "Although the title may immediately evoke the thought of a conventional least squares regression model, the term regression here is a more general one that simply means to say that the target/outcome is lies on a continuum. That is, the targets we will try to predict or estimate are not binary nor discrete in nature.\n",
        "\n",
        "### Step 1. Load and explore the data\n",
        "As an example, let's look at the following data on housing prices. The target/outcome that we will want to later on predict is the house price itself. Let's load the data and start to explore.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5H_lvdcQe4V"
      },
      "source": [
        "#Importing proper packages as thier common nomenclatures\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#Takes in CSV file as \"DATA_URL\"\n",
        "DATA_URL = \"https://raw.githubusercontent.com/sgeinitz/cs39aa_data/main/houseprices.csv\"\n",
        "\n",
        "#pandas package reads CSV as \"Train_df\"\n",
        "train_df = pd.read_csv(DATA_URL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFXl_g94Qe4X"
      },
      "source": [
        "The pandas library has some useful functionality for managing datasets. The basic data type used is a pandas DataFrame, which we can confirm with the following."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqKyysrfQe4Y",
        "outputId": "10e0e41b-bfd7-4959-bf72-a1f9fcd4e0b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#if the next line is commented out, the output is \"pandas.core.frame.Dataframe\"\n",
        "#Also, \"type(train_df.dtypes)\" ran alone; Output = \"pandas.core.series.series\"\n",
        "#The output with \"type(train_df.dtypes)\" along with the below line uncommented -\n",
        "#- still produces the same output as the two original, unaltered codes ran -\n",
        "#- togeher, which is the same output as the second original line, -\n",
        "#-\"train_df.dtypes\" ran by itself\n",
        "type(train_df.dtypes)\n",
        "\n",
        "#with the above line commented out, the output is the same as if neither were -\n",
        "# - were commented out.\n",
        "train_df.dtypes\n",
        "\n",
        "#side note not sure if the output is this way because im using collab? I will be\n",
        "#- looking into this. ive used pandas, numpy matplotlib etc in the past and ive-\n",
        "#- noticed some of the imports can be slightly different to use on collab; -\n",
        "#- for instance in CV with Dr. Fieng I found out the RGB is backwards and needs-\n",
        "#- to be corrected."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Id                 int64\n",
              "MSSubClass         int64\n",
              "MSZoning          object\n",
              "LotFrontage      float64\n",
              "LotArea            int64\n",
              "                  ...   \n",
              "MoSold             int64\n",
              "YrSold             int64\n",
              "SaleType          object\n",
              "SaleCondition     object\n",
              "SalePrice          int64\n",
              "Length: 81, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDCQabL6Qe4Y"
      },
      "source": [
        "Recall that Python is very much follows an object-oriented programming paradigm, and pandas is no exception to that. A pandas DataFrame has both attributes/fields as well as methods that can be called. One attribute/field that is useful to look is `shape`, which tells us how many rows and columns a DataFrame has."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dol0p5X5Qe4Y",
        "outputId": "d926e4e0-5d8f-4739-b5f2-2820708e3225",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 'shape' of a DataFrame is it's number of rows and columns\n",
        "#this pretty much does what we expect. This line prints the shape with a label\n",
        "#that lets us know what is is\n",
        "print(\"train_df.shape: \", train_df.shape)\n",
        "\n",
        "# more specifically...\n",
        "#concatonation using \"print(f\"...\"..\"shape\" of rows starting at 0, while-\n",
        "#- the \"shape\" of the columns start at 1 give the count for both the rows and -\n",
        "#- columns so the output is more user friendly. Excersize meant to demonstrate-\n",
        "#- ability to begin navigating the data from the .csv\n",
        "print(f\"train_df has {train_df.shape[0]} rows and {train_df.shape[1]} columns\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_df.shape:  (1460, 81)\n",
            "train_df has 1460 rows and 81 columns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_U_vQFXQe4Z"
      },
      "source": [
        "To see what all of the columns names are we can also print out the `columns` attribute/field of the DataFrame object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cel66meQe4b",
        "outputId": "5cc42b41-c2c1-4ea6-ff21-141e7191f3a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_df.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Id', 'MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',\n",
              "       'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig',\n",
              "       'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n",
              "       'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',\n",
              "       'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n",
              "       'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual',\n",
              "       'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1',\n",
              "       'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating',\n",
              "       'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF',\n",
              "       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n",
              "       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual',\n",
              "       'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType',\n",
              "       'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual',\n",
              "       'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF',\n",
              "       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC',\n",
              "       'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType',\n",
              "       'SaleCondition', 'SalePrice'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7yQOu9rQe4b"
      },
      "source": [
        "Let's continue exploring the housing prices dataset using the pandas `describe()` function/method. This will summarize each column using common descriptive statistics (i.e. mean, stdev, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TM7huDbKQe4c",
        "outputId": "2e2a41a3-d041-4957-b7d2-0e86dd885284",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "train_df.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>MSSubClass</th>\n",
              "      <th>LotFrontage</th>\n",
              "      <th>LotArea</th>\n",
              "      <th>OverallQual</th>\n",
              "      <th>OverallCond</th>\n",
              "      <th>YearBuilt</th>\n",
              "      <th>YearRemodAdd</th>\n",
              "      <th>MasVnrArea</th>\n",
              "      <th>BsmtFinSF1</th>\n",
              "      <th>BsmtFinSF2</th>\n",
              "      <th>BsmtUnfSF</th>\n",
              "      <th>TotalBsmtSF</th>\n",
              "      <th>1stFlrSF</th>\n",
              "      <th>2ndFlrSF</th>\n",
              "      <th>LowQualFinSF</th>\n",
              "      <th>GrLivArea</th>\n",
              "      <th>BsmtFullBath</th>\n",
              "      <th>BsmtHalfBath</th>\n",
              "      <th>FullBath</th>\n",
              "      <th>HalfBath</th>\n",
              "      <th>BedroomAbvGr</th>\n",
              "      <th>KitchenAbvGr</th>\n",
              "      <th>TotRmsAbvGrd</th>\n",
              "      <th>Fireplaces</th>\n",
              "      <th>GarageYrBlt</th>\n",
              "      <th>GarageCars</th>\n",
              "      <th>GarageArea</th>\n",
              "      <th>WoodDeckSF</th>\n",
              "      <th>OpenPorchSF</th>\n",
              "      <th>EnclosedPorch</th>\n",
              "      <th>3SsnPorch</th>\n",
              "      <th>ScreenPorch</th>\n",
              "      <th>PoolArea</th>\n",
              "      <th>MiscVal</th>\n",
              "      <th>MoSold</th>\n",
              "      <th>YrSold</th>\n",
              "      <th>SalePrice</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1460.000000</td>\n",
              "      <td>1460.000000</td>\n",
              "      <td>1201.000000</td>\n",
              "      <td>1460.000000</td>\n",
              "      <td>1460.000000</td>\n",
              "      <td>1460.000000</td>\n",
              "      <td>1460.000000</td>\n",
              "      <td>1460.000000</td>\n",
              "      <td>1452.000000</td>\n",
              "      <td>1460.000000</td>\n",
              "      <td>1460.000000</td>\n",
              "      <td>1460.000000</td>\n",
              "      <td>1460.000000</td>\n",
              "      <td>1460.000000</td>\n",
              "      <td>1460.000000</td>\n",
              "      <td>1460.000000</td>\n",
              "      <td>1460.000000</td>\n",
              "      <td>1460.000000</td>\n",
              "      <td>1460.000000</td>\n",
              "      <td>1460.000000</td>\n",
              "      <td>1460.000000</td>\n",
              "      <td>1460.000000</td>\n",
              "      <td>1460.000000</td>\n",
              "      <td>1460.000000</td>\n",
              "      <td>1460.000000</td>\n",
              "      <td>1379.000000</td>\n",
              "      <td>1460.000000</td>\n",
              "      <td>1460.000000</td>\n",
              "      <td>1460.000000</td>\n",
              "      <td>1460.000000</td>\n",
              "      <td>1460.000000</td>\n",
              "      <td>1460.000000</td>\n",
              "      <td>1460.000000</td>\n",
              "      <td>1460.000000</td>\n",
              "      <td>1460.000000</td>\n",
              "      <td>1460.000000</td>\n",
              "      <td>1460.000000</td>\n",
              "      <td>1460.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>730.500000</td>\n",
              "      <td>56.897260</td>\n",
              "      <td>70.049958</td>\n",
              "      <td>10516.828082</td>\n",
              "      <td>6.099315</td>\n",
              "      <td>5.575342</td>\n",
              "      <td>1971.267808</td>\n",
              "      <td>1984.865753</td>\n",
              "      <td>103.685262</td>\n",
              "      <td>443.639726</td>\n",
              "      <td>46.549315</td>\n",
              "      <td>567.240411</td>\n",
              "      <td>1057.429452</td>\n",
              "      <td>1162.626712</td>\n",
              "      <td>346.992466</td>\n",
              "      <td>5.844521</td>\n",
              "      <td>1515.463699</td>\n",
              "      <td>0.425342</td>\n",
              "      <td>0.057534</td>\n",
              "      <td>1.565068</td>\n",
              "      <td>0.382877</td>\n",
              "      <td>2.866438</td>\n",
              "      <td>1.046575</td>\n",
              "      <td>6.517808</td>\n",
              "      <td>0.613014</td>\n",
              "      <td>1978.506164</td>\n",
              "      <td>1.767123</td>\n",
              "      <td>472.980137</td>\n",
              "      <td>94.244521</td>\n",
              "      <td>46.660274</td>\n",
              "      <td>21.954110</td>\n",
              "      <td>3.409589</td>\n",
              "      <td>15.060959</td>\n",
              "      <td>2.758904</td>\n",
              "      <td>43.489041</td>\n",
              "      <td>6.321918</td>\n",
              "      <td>2007.815753</td>\n",
              "      <td>180921.195890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>421.610009</td>\n",
              "      <td>42.300571</td>\n",
              "      <td>24.284752</td>\n",
              "      <td>9981.264932</td>\n",
              "      <td>1.382997</td>\n",
              "      <td>1.112799</td>\n",
              "      <td>30.202904</td>\n",
              "      <td>20.645407</td>\n",
              "      <td>181.066207</td>\n",
              "      <td>456.098091</td>\n",
              "      <td>161.319273</td>\n",
              "      <td>441.866955</td>\n",
              "      <td>438.705324</td>\n",
              "      <td>386.587738</td>\n",
              "      <td>436.528436</td>\n",
              "      <td>48.623081</td>\n",
              "      <td>525.480383</td>\n",
              "      <td>0.518911</td>\n",
              "      <td>0.238753</td>\n",
              "      <td>0.550916</td>\n",
              "      <td>0.502885</td>\n",
              "      <td>0.815778</td>\n",
              "      <td>0.220338</td>\n",
              "      <td>1.625393</td>\n",
              "      <td>0.644666</td>\n",
              "      <td>24.689725</td>\n",
              "      <td>0.747315</td>\n",
              "      <td>213.804841</td>\n",
              "      <td>125.338794</td>\n",
              "      <td>66.256028</td>\n",
              "      <td>61.119149</td>\n",
              "      <td>29.317331</td>\n",
              "      <td>55.757415</td>\n",
              "      <td>40.177307</td>\n",
              "      <td>496.123024</td>\n",
              "      <td>2.703626</td>\n",
              "      <td>1.328095</td>\n",
              "      <td>79442.502883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>1300.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1872.000000</td>\n",
              "      <td>1950.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>334.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>334.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1900.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2006.000000</td>\n",
              "      <td>34900.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>365.750000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>59.000000</td>\n",
              "      <td>7553.500000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>1954.000000</td>\n",
              "      <td>1967.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>223.000000</td>\n",
              "      <td>795.750000</td>\n",
              "      <td>882.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1129.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1961.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>334.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>2007.000000</td>\n",
              "      <td>129975.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>730.500000</td>\n",
              "      <td>50.000000</td>\n",
              "      <td>69.000000</td>\n",
              "      <td>9478.500000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>1973.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>383.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>477.500000</td>\n",
              "      <td>991.500000</td>\n",
              "      <td>1087.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1464.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1980.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>480.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>2008.000000</td>\n",
              "      <td>163000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1095.250000</td>\n",
              "      <td>70.000000</td>\n",
              "      <td>80.000000</td>\n",
              "      <td>11601.500000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2004.000000</td>\n",
              "      <td>166.000000</td>\n",
              "      <td>712.250000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>808.000000</td>\n",
              "      <td>1298.250000</td>\n",
              "      <td>1391.250000</td>\n",
              "      <td>728.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1776.750000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2002.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>576.000000</td>\n",
              "      <td>168.000000</td>\n",
              "      <td>68.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>2009.000000</td>\n",
              "      <td>214000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1460.000000</td>\n",
              "      <td>190.000000</td>\n",
              "      <td>313.000000</td>\n",
              "      <td>215245.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>2010.000000</td>\n",
              "      <td>2010.000000</td>\n",
              "      <td>1600.000000</td>\n",
              "      <td>5644.000000</td>\n",
              "      <td>1474.000000</td>\n",
              "      <td>2336.000000</td>\n",
              "      <td>6110.000000</td>\n",
              "      <td>4692.000000</td>\n",
              "      <td>2065.000000</td>\n",
              "      <td>572.000000</td>\n",
              "      <td>5642.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2010.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1418.000000</td>\n",
              "      <td>857.000000</td>\n",
              "      <td>547.000000</td>\n",
              "      <td>552.000000</td>\n",
              "      <td>508.000000</td>\n",
              "      <td>480.000000</td>\n",
              "      <td>738.000000</td>\n",
              "      <td>15500.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>2010.000000</td>\n",
              "      <td>755000.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Id   MSSubClass  ...       YrSold      SalePrice\n",
              "count  1460.000000  1460.000000  ...  1460.000000    1460.000000\n",
              "mean    730.500000    56.897260  ...  2007.815753  180921.195890\n",
              "std     421.610009    42.300571  ...     1.328095   79442.502883\n",
              "min       1.000000    20.000000  ...  2006.000000   34900.000000\n",
              "25%     365.750000    20.000000  ...  2007.000000  129975.000000\n",
              "50%     730.500000    50.000000  ...  2008.000000  163000.000000\n",
              "75%    1095.250000    70.000000  ...  2009.000000  214000.000000\n",
              "max    1460.000000   190.000000  ...  2010.000000  755000.000000\n",
              "\n",
              "[8 rows x 38 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOhxUo3oQe4c"
      },
      "source": [
        "Notice that with `describe()` it says there are only 38 columns even though train_df has 81 columns. That's because only the numeric columns had meaningful descriptive statistics. To see non-numeric columns as well we can simply print out the first few rows of a DataFrame using `head()`, although note that even though it now says there are 81 columns we only see the first 10 and last 10."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1vxnJSLQe4d",
        "outputId": "e3734b11-8a71-4de8-bb87-40d8aa6ed1ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        }
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>MSSubClass</th>\n",
              "      <th>MSZoning</th>\n",
              "      <th>LotFrontage</th>\n",
              "      <th>LotArea</th>\n",
              "      <th>Street</th>\n",
              "      <th>Alley</th>\n",
              "      <th>LotShape</th>\n",
              "      <th>LandContour</th>\n",
              "      <th>Utilities</th>\n",
              "      <th>LotConfig</th>\n",
              "      <th>LandSlope</th>\n",
              "      <th>Neighborhood</th>\n",
              "      <th>Condition1</th>\n",
              "      <th>Condition2</th>\n",
              "      <th>BldgType</th>\n",
              "      <th>HouseStyle</th>\n",
              "      <th>OverallQual</th>\n",
              "      <th>OverallCond</th>\n",
              "      <th>YearBuilt</th>\n",
              "      <th>YearRemodAdd</th>\n",
              "      <th>RoofStyle</th>\n",
              "      <th>RoofMatl</th>\n",
              "      <th>Exterior1st</th>\n",
              "      <th>Exterior2nd</th>\n",
              "      <th>MasVnrType</th>\n",
              "      <th>MasVnrArea</th>\n",
              "      <th>ExterQual</th>\n",
              "      <th>ExterCond</th>\n",
              "      <th>Foundation</th>\n",
              "      <th>BsmtQual</th>\n",
              "      <th>BsmtCond</th>\n",
              "      <th>BsmtExposure</th>\n",
              "      <th>BsmtFinType1</th>\n",
              "      <th>BsmtFinSF1</th>\n",
              "      <th>BsmtFinType2</th>\n",
              "      <th>BsmtFinSF2</th>\n",
              "      <th>BsmtUnfSF</th>\n",
              "      <th>TotalBsmtSF</th>\n",
              "      <th>Heating</th>\n",
              "      <th>...</th>\n",
              "      <th>CentralAir</th>\n",
              "      <th>Electrical</th>\n",
              "      <th>1stFlrSF</th>\n",
              "      <th>2ndFlrSF</th>\n",
              "      <th>LowQualFinSF</th>\n",
              "      <th>GrLivArea</th>\n",
              "      <th>BsmtFullBath</th>\n",
              "      <th>BsmtHalfBath</th>\n",
              "      <th>FullBath</th>\n",
              "      <th>HalfBath</th>\n",
              "      <th>BedroomAbvGr</th>\n",
              "      <th>KitchenAbvGr</th>\n",
              "      <th>KitchenQual</th>\n",
              "      <th>TotRmsAbvGrd</th>\n",
              "      <th>Functional</th>\n",
              "      <th>Fireplaces</th>\n",
              "      <th>FireplaceQu</th>\n",
              "      <th>GarageType</th>\n",
              "      <th>GarageYrBlt</th>\n",
              "      <th>GarageFinish</th>\n",
              "      <th>GarageCars</th>\n",
              "      <th>GarageArea</th>\n",
              "      <th>GarageQual</th>\n",
              "      <th>GarageCond</th>\n",
              "      <th>PavedDrive</th>\n",
              "      <th>WoodDeckSF</th>\n",
              "      <th>OpenPorchSF</th>\n",
              "      <th>EnclosedPorch</th>\n",
              "      <th>3SsnPorch</th>\n",
              "      <th>ScreenPorch</th>\n",
              "      <th>PoolArea</th>\n",
              "      <th>PoolQC</th>\n",
              "      <th>Fence</th>\n",
              "      <th>MiscFeature</th>\n",
              "      <th>MiscVal</th>\n",
              "      <th>MoSold</th>\n",
              "      <th>YrSold</th>\n",
              "      <th>SaleType</th>\n",
              "      <th>SaleCondition</th>\n",
              "      <th>SalePrice</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>65.0</td>\n",
              "      <td>8450</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Reg</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>Inside</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>CollgCr</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>2Story</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>2003</td>\n",
              "      <td>2003</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>BrkFace</td>\n",
              "      <td>196.0</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>PConc</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>No</td>\n",
              "      <td>GLQ</td>\n",
              "      <td>706</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>856</td>\n",
              "      <td>GasA</td>\n",
              "      <td>...</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>856</td>\n",
              "      <td>854</td>\n",
              "      <td>0</td>\n",
              "      <td>1710</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>8</td>\n",
              "      <td>Typ</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>2003.0</td>\n",
              "      <td>RFn</td>\n",
              "      <td>2</td>\n",
              "      <td>548</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>0</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>208500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>20</td>\n",
              "      <td>RL</td>\n",
              "      <td>80.0</td>\n",
              "      <td>9600</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Reg</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>FR2</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>Veenker</td>\n",
              "      <td>Feedr</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>1Story</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>1976</td>\n",
              "      <td>1976</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>MetalSd</td>\n",
              "      <td>MetalSd</td>\n",
              "      <td>None</td>\n",
              "      <td>0.0</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>CBlock</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>Gd</td>\n",
              "      <td>ALQ</td>\n",
              "      <td>978</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0</td>\n",
              "      <td>284</td>\n",
              "      <td>1262</td>\n",
              "      <td>GasA</td>\n",
              "      <td>...</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>1262</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1262</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>TA</td>\n",
              "      <td>6</td>\n",
              "      <td>Typ</td>\n",
              "      <td>1</td>\n",
              "      <td>TA</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>1976.0</td>\n",
              "      <td>RFn</td>\n",
              "      <td>2</td>\n",
              "      <td>460</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>298</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>2007</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>181500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>68.0</td>\n",
              "      <td>11250</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>Inside</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>CollgCr</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>2Story</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>2001</td>\n",
              "      <td>2002</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>BrkFace</td>\n",
              "      <td>162.0</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>PConc</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>Mn</td>\n",
              "      <td>GLQ</td>\n",
              "      <td>486</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0</td>\n",
              "      <td>434</td>\n",
              "      <td>920</td>\n",
              "      <td>GasA</td>\n",
              "      <td>...</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>920</td>\n",
              "      <td>866</td>\n",
              "      <td>0</td>\n",
              "      <td>1786</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>6</td>\n",
              "      <td>Typ</td>\n",
              "      <td>1</td>\n",
              "      <td>TA</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>2001.0</td>\n",
              "      <td>RFn</td>\n",
              "      <td>2</td>\n",
              "      <td>608</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>0</td>\n",
              "      <td>42</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>223500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>70</td>\n",
              "      <td>RL</td>\n",
              "      <td>60.0</td>\n",
              "      <td>9550</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>Corner</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>Crawfor</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>2Story</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>1915</td>\n",
              "      <td>1970</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>Wd Sdng</td>\n",
              "      <td>Wd Shng</td>\n",
              "      <td>None</td>\n",
              "      <td>0.0</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>BrkTil</td>\n",
              "      <td>TA</td>\n",
              "      <td>Gd</td>\n",
              "      <td>No</td>\n",
              "      <td>ALQ</td>\n",
              "      <td>216</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0</td>\n",
              "      <td>540</td>\n",
              "      <td>756</td>\n",
              "      <td>GasA</td>\n",
              "      <td>...</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>961</td>\n",
              "      <td>756</td>\n",
              "      <td>0</td>\n",
              "      <td>1717</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>7</td>\n",
              "      <td>Typ</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>Detchd</td>\n",
              "      <td>1998.0</td>\n",
              "      <td>Unf</td>\n",
              "      <td>3</td>\n",
              "      <td>642</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>0</td>\n",
              "      <td>35</td>\n",
              "      <td>272</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2006</td>\n",
              "      <td>WD</td>\n",
              "      <td>Abnorml</td>\n",
              "      <td>140000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>84.0</td>\n",
              "      <td>14260</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>FR2</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>NoRidge</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>2Story</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>2000</td>\n",
              "      <td>2000</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>BrkFace</td>\n",
              "      <td>350.0</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>PConc</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>Av</td>\n",
              "      <td>GLQ</td>\n",
              "      <td>655</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0</td>\n",
              "      <td>490</td>\n",
              "      <td>1145</td>\n",
              "      <td>GasA</td>\n",
              "      <td>...</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>1145</td>\n",
              "      <td>1053</td>\n",
              "      <td>0</td>\n",
              "      <td>2198</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>9</td>\n",
              "      <td>Typ</td>\n",
              "      <td>1</td>\n",
              "      <td>TA</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>RFn</td>\n",
              "      <td>3</td>\n",
              "      <td>836</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>192</td>\n",
              "      <td>84</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>250000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 81 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Id  MSSubClass MSZoning  ...  SaleType  SaleCondition SalePrice\n",
              "0   1          60       RL  ...        WD         Normal    208500\n",
              "1   2          20       RL  ...        WD         Normal    181500\n",
              "2   3          60       RL  ...        WD         Normal    223500\n",
              "3   4          70       RL  ...        WD        Abnorml    140000\n",
              "4   5          60       RL  ...        WD         Normal    250000\n",
              "\n",
              "[5 rows x 81 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZWaf10xQe4e"
      },
      "source": [
        "We can start to visually explore some of the individual columns/fields now, starting with the target variable, `SalePrice`.\n",
        "\n",
        "We'll need `maplotlib` for this so we will import that first, and then plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNgnzXKVQe4f",
        "outputId": "41e5fa52-4617-4986-c92b-f5458c98bcef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.hist(train_df['SalePrice'], bins=20, edgecolor='black')\n",
        "plt.xlabel(\"home sale price\")\n",
        "plt.ylabel(\"# of observations\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6Ids5tKQe4g"
      },
      "source": [
        "We can also start to explore some of the relationships between the features (i.e. independent variables) and the target (i.e. dependent variable)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Qwl4czzQe4i",
        "outputId": "394edce2-dc0e-4132-ab55-1385c0f329e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "plt.scatter(train_df['1stFlrSF'], train_df['SalePrice'], alpha=.2)\n",
        "#plt.xlabel(\"first floor square footage\")\n",
        "#plt.ylabel(\"home sale price\")\n",
        "#plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f2871c83ed0>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNMSESezQe4j"
      },
      "source": [
        "Now let's look at some of the categorical/discrete features, such as `SaleCondition`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIVwiP91Qe4j"
      },
      "source": [
        "plt.hist(train_df['SaleCondition'])\n",
        "plt.xlabel(\"condition at time of sale\")\n",
        "plt.ylabel(\"# of observations\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7uy7RfdQe4k"
      },
      "source": [
        "Before we can go much further we'll have to deal with some of the missing values in the data, so let's do that now with our DataFrame object's `isnull()` method, and then `sum()` method to add up the number of null entries in each column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx0DW_mFQe4l"
      },
      "source": [
        "null_entries = train_df.isnull().sum()\n",
        "null_entries"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFkh1g0TQe4l"
      },
      "source": [
        "It looks like most of the columns have 0 missing/na values, which is good. Let's try looking at just the ones with at least 1 missing/na value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQfTyVfWQe4l"
      },
      "source": [
        "null_entries[null_entries > 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8mgi_38Qe4n"
      },
      "source": [
        "There are some columns with many missing values, although most of them are only missing a few.\n",
        "\n",
        "If we were working on a real project using this data then we would likely want to see whether those columns with just a few missing values are worth trying to salvage in some way (e.g. interpolation). For our purposes we will simply drop all of these columns from our dataset.\n",
        "\n",
        "We'll then convert the categorial fields (that are strings) to a numerical format and double check the data type of all of the columns in `X`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v33fZOhnQe4n"
      },
      "source": [
        "# Create temporary y=target and X=features, excluding those cols with missing values from X\n",
        "cols_w_nulls = null_entries[null_entries > 0].index\n",
        "X = train_df.drop(columns=cols_w_nulls).copy()\n",
        "y = X.pop(\"SalePrice\")\n",
        "\n",
        "# Label encoding for categoricals\n",
        "for colname in X.select_dtypes('object').columns:\n",
        "    X[colname], _ = X[colname].factorize()\n",
        "\n",
        "# Check data type of each column in X\n",
        "X.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYdRgWEiQe4o"
      },
      "source": [
        "### 2. Selecting features for a model\n",
        "\n",
        "There are many ways to approach feature selection but we will quickly look at one using mutual information between each individual feature and the target (we won't dig into what mutual information is exactly is in this course, but you can think of it as a the difference between the probability distribution of a variable, X, versus the conditional probability distribution of X given Y - i.e. how much additional information does Y provide about X).\n",
        "\n",
        "Let's create a simple wrapper around the scikit learn mutual information function to store and sort the results, which we can then plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J7dKYNJQe4o"
      },
      "source": [
        "from sklearn.feature_selection import mutual_info_regression\n",
        "\n",
        "def make_mi_scores(X, y):\n",
        "    assert X.shape[0] == y.shape[0] and X.shape[1] > 1\n",
        "    mi_scores = mutual_info_regression(X, y)\n",
        "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
        "    mi_scores = mi_scores.sort_values(ascending=False)\n",
        "    return mi_scores\n",
        "\n",
        "mi_scores = make_mi_scores(X, y)#, discrete_features)\n",
        "\n",
        "# Look at every fifth score to see the range of values\n",
        "# Note: the range of possible values is positive reals, but in practice it rarely goes above 1.0 or 2.0\n",
        "mi_scores[:11]\n",
        "\n",
        "def plot_mi_scores(scores):\n",
        "    scores = scores.sort_values(ascending=True)\n",
        "    width = np.arange(len(scores))\n",
        "    ticks = list(scores.index)\n",
        "    plt.barh(width, scores)\n",
        "    plt.yticks(width, ticks)\n",
        "    plt.title(\"Mutual Information Scores\")\n",
        "\n",
        "# plot the top 20 scores\n",
        "plt.figure(dpi=100, figsize=(8, 5))\n",
        "plot_mi_scores(mi_scores[:25])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuVH7i56Qe4p"
      },
      "source": [
        "### 3. Train a model\n",
        "\n",
        "Let's start with a basic model, a linear regression model.\n",
        "\n",
        "Also, let's assume that we want to use only the most simple, objective, and unambiguous features. For example, the most informative feature, `OverallQual`, is a feature that was derived in some way that required human input. If we want to take only the features that can be quickly and precisely used, then `OverallQual` would not be one of them. However, `GarageCars`, and `1stFlrSF` would be. Let's take a set of ~10 such features and use those in our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioUUVUT1Qe4q"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# top informative features that are also easy/fast/precise to measure\n",
        "selected_features = ['GrLivArea', 'GarageCars', 'GarageArea', 'YearBuilt', 'TotalBsmtSF', '1stFlrSF', 'FullBath', 'TotRmsAbvGrd', 'Fireplaces']\n",
        "X_linreg = X[selected_features].copy()\n",
        "\n",
        "# declare and fit (i.e. train) a model\n",
        "linreg_model = LinearRegression()\n",
        "linreg_model.fit(X_linreg, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGBN8G7oQe4q"
      },
      "source": [
        "We need to have a way to assess the fit/performance of this model.\n",
        "\n",
        "What is the standard way to assess a least squares linear regression model? _note the hint given there!_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ix66KDB0Qe4s"
      },
      "source": [
        "# Calculate predicted y's usign the linear regression model\n",
        "y_pred = linreg_model.predict(X_linreg)\n",
        "\n",
        "# Plot the predictions side-by-side with the actual values\n",
        "# Note: a perfect model would yield the identity, y=x\n",
        "plt.scatter(y, y_pred, alpha=0.3)\n",
        "plt.xlabel('y actual')\n",
        "plt.ylabel('y predicted')\n",
        "plt.plot([0,5e5],[0,5e5], c='k', alpha=0.2)\n",
        "plt.show()\n",
        "\n",
        "# Calculate and show the mean squared error\n",
        "mse_linreg = (((y - y_pred)**2).mean())\n",
        "print(\"mse_linreg = \", mse_linreg)\n",
        "print(\"rmse_linreg = \", np.sqrt(mse_linreg))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFJKeoSzQe4t"
      },
      "source": [
        "\n",
        "### 4. Feature Engineering\n",
        "\n",
        "In the field of Data Science it is commonly said that 80-90% of the time is spent on cleaning and munging the data. Part of this process involves looking at the features and trying to derive new ones that can be used more readily by your model. In the case of linear regression we know that only linear relationships of the features are considered.\n",
        "\n",
        "That's not to say that a more complex relationship cannot be modeled, but we must manipulate and transform the features to allow a linear regression model to take advantage of it.\n",
        "\n",
        "For example, the true relationship between $y$ and $x$ may be quadratic in nature, i.e.:\n",
        "  * $y = a * x^2 + b * x + ...$.\n",
        "\n",
        "If, however, we only observe $x$, then we can simply create a new feature, $x^{'} = x * x$.\n",
        "\n",
        "Often times new features are combinations of other features themselves, since most models cannot account for such interactions on their own.\n",
        "\n",
        "In our case, suppose that we see the ratio of `GarageArea` to `1stFlrSF` helps our model perform better. Let's create this now and see how our model does.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuLPN0_SQe4u"
      },
      "source": [
        "X_linreg['Garage1stFlrRatio'] = (X_linreg['GarageArea']/X_linreg['1stFlrSF'])\n",
        "#plt.scatter(X_linreg['Garage1stFlrRatio'], y, alpha=0.2)\n",
        "#plt.show()\n",
        "\n",
        "linreg_model.fit(X_linreg, y)\n",
        "\n",
        "# Calculate predicted y's usign the linear regression model\n",
        "y_pred = linreg_model.predict(X_linreg)\n",
        "\n",
        "# Plot the predictions side-by-side with the actual values\n",
        "# Note: a perfect model would yield the identity, y=x\n",
        "plt.scatter(y, y_pred, alpha=0.3)\n",
        "plt.xlabel('y actual')\n",
        "plt.ylabel('y predicted')\n",
        "plt.plot([0,5e5],[0,5e5], c='k', alpha=0.2)\n",
        "plt.show()\n",
        "\n",
        "# Calculate and show the mean squared error\n",
        "mse_linreg = (((y - y_pred)**2).mean())\n",
        "print(\"mse_linreg = \", mse_linreg)\n",
        "print(\"rmse_linreg = \", np.sqrt(mse_linreg))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSX9wKJUQe4x"
      },
      "source": [
        "Adding that feature is only a modest improvement in our case (~5% reduction in MSE), but interactions are often present in data yet difficult to incorporate for some types of models, particularly for linear regression.\n",
        "\n",
        "### 5. Decision Tree Regressor\n",
        "Now we will try another traditional ML model, namely a decision tree."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQsO7w1UQe4z"
      },
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "dectree_model = DecisionTreeRegressor(criterion='mse')\n",
        "dectree_model.fit(X_linreg, y)\n",
        "y_pred = dectree_model.predict(X_linreg)\n",
        "\n",
        "# Plot the predictions side-by-side with the actual values\n",
        "# Note: a perfect model would yield the identity, y=x\n",
        "plt.scatter(y, y_pred, alpha=0.3)\n",
        "plt.xlabel('y actual')\n",
        "plt.ylabel('y predicted')\n",
        "plt.plot([0,5e5],[0,5e5], c='k', alpha=0.2)\n",
        "plt.show()\n",
        "\n",
        "# Calculate and show the mean squared error\n",
        "mse_linreg = (((y - y_pred)**2).mean())\n",
        "print(\"mse_linreg = \", mse_linreg)\n",
        "print(\"rmse_linreg = \", np.sqrt(mse_linreg))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJIk07KUQe42"
      },
      "source": [
        "Wait a second, what happened there? We have a nearly perfect model!\n",
        "\n",
        "Is there anything we should be concerned about before we begin using this model for our new Zillow-type business endeavor?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA3BUXLRQe43"
      },
      "source": [
        "### 6. Overfitting / Underfitting and the Train-Validate-Test Process\n",
        "\n",
        "Yes, the issue with the decision tree we used above is that there was no maximum depth set for our tree, so it kept creating branches until it perfectly learned the dataset.\n",
        "\n",
        "We can verify this by looking at the MSE for another dataset that was not used to train/fit the model. We'll have to separate this from the original dataset, and refit each model. Once we do though, we can compare the linear regression and decision tree model using this held-out test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzA5FSY9Qe44"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create the train and test splits\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_linreg, y, test_size=0.20, random_state=1)\n",
        "\n",
        "# Fit lin regression and calculate predicted test y's\n",
        "linreg_model.fit(X_train, y_train)\n",
        "y_linreg_pred = linreg_model.predict(X_test)\n",
        "\n",
        "# Fit lin regression and calculate predicted test y's\n",
        "dectree_model = DecisionTreeRegressor(criterion='mse')\n",
        "dectree_model.fit(X_train, y_train)\n",
        "y_dectree_pred = dectree_model.predict(X_test)\n",
        "\n",
        "# Plot the predictions side-by-side with the actual values\n",
        "# Note: a perfect model would yield the identity, y=x\n",
        "plt.scatter(y_test, y_linreg_pred, alpha=0.2, c='b')\n",
        "plt.scatter(y_test, y_dectree_pred, alpha=0.2, c='r')\n",
        "plt.legend(['lin reg test predictions', 'dec tree test predictions'])\n",
        "plt.xlabel('y test actual')\n",
        "plt.ylabel('y test predicted')\n",
        "plt.plot([0,5e5],[0,5e5], c='k', alpha=0.2)\n",
        "plt.show()\n",
        "\n",
        "# Calculate and show the mean squared errors\n",
        "mse_linreg = (((y_test - y_linreg_pred)**2).mean())\n",
        "print(f\"linreg mse = {mse_linreg:.2f} (rmse = {np.sqrt(mse_linreg):.2f})\")\n",
        "\n",
        "mse_dectree = (((y_test - y_dectree_pred)**2).mean())\n",
        "print(f\"dectree mse = {mse_dectree:.2f} (rmse = {np.sqrt(mse_dectree):.2f})\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sao4iLVQe44"
      },
      "source": [
        "The resulting MSEs for the test datasets are much more comparable now, which restores some of our confidence in usefulness of basic linear regression.\n",
        "\n",
        "It's likely that we can lower the MSE of the decision tree model though by preventing it from overfitting. One of the decision tree model parameters that can be used to prevent overfitting is the maximum depth of the tree (`max_depth` is the parameter we can pass to the function).\n",
        "\n",
        "Try changing the `max_depth` and see what happens. Note that a higher maximum depth will cause the model to conform more to the training dataset, while a lower maximum depth will limit how much it conforms to the training data.\n",
        "\n",
        "Often it is helpful to vary model parameters and calculate the loss (e.g. MSE) using the validation/test for each value of the model parameter. It's then possible to see how the test/validation loss changes with parameter value, and to find the optimal parameter value for your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iF_Ovod8Qe46"
      },
      "source": [
        "## II. Classification\n",
        "\n",
        "### 1. Titanic dataset\n",
        "\n",
        "Let's now look at a classification problem using a dataset where each row represents a passenger on the Titanic. This target/outcome is whether the passenger survived the voyage or not, so it is binary in nature. The features/predictors include the following:\n",
        "\n",
        "* survived: this is the target/outcome we are interested in predicting\n",
        "* pclass: A proxy for socio-economic status (SES)\n",
        "  - 1st = Upper\n",
        "  - 2nd = Middle\n",
        "  - 3rd = Lower\n",
        "* name\n",
        "* sex\n",
        "* age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n",
        "* sibsp: The dataset defines family relations in this way...\n",
        "  - Sibling = brother, sister, stepbrother, stepsister\n",
        "  - Spouse = husband, wife (mistresses and fiancés were ignored)\n",
        "* parch: The dataset defines family relations in this way...\n",
        "  - Parent = mother, father\n",
        "  - Child = daughter, son, stepdaughter, stepson\n",
        "  - (some children travelled only with a nanny, therefore parch=0 for them)\n",
        "* ticket: ticket number\n",
        "* fare: ticket price\n",
        "* cabin: cabin number\n",
        "* embarked:\n",
        "  - C = Cherbourg\n",
        "  - Q = Queenstown\n",
        "  - S = Southampton\n",
        "\n",
        "\n",
        "\n",
        "Note that the use of this dataset as an example in Data Science/ML is common, but worth considering. As a prediction task, it doesn't make much sense for us to try to _predict_ survival for such a unique historical event. However, as a modeling task to better understand what features are most were most influential in predicting survival, it is an interesting and worthwhile dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiYZLGhRQe47"
      },
      "source": [
        "DATA_URL = \"https://raw.githubusercontent.com/sgeinitz/cs39aa_data/main/titanic.csv\"\n",
        "\n",
        "train_df = pd.read_csv(DATA_URL)\n",
        "print(\"train_df.shape: \", train_df.shape)\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSNeGzbwQe48"
      },
      "source": [
        "train_df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hi_f-BgOQe5B"
      },
      "source": [
        "As before, there are again some missing values to contend with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCfC0QLJQe5C"
      },
      "source": [
        "null_entries = train_df.isnull().sum()\n",
        "null_entries"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMc4J029Qe5D"
      },
      "source": [
        "Cabin number is missing so many values (~80% of the observations), that it's probably best to drop the entire column.\n",
        "\n",
        "Age seems like an important feature so that is something we would normally want to try to keep by using interpolation, or some other method. For our example though, we will simply drop all observations with a missing `Age` value, as well as the ones with missing `Embarked` values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc_VMhYEQe5F"
      },
      "source": [
        "# remove cabin columns\n",
        "train_df.drop(columns=[\"Cabin\"], inplace=True)\n",
        "\n",
        "# remove rows with missing Age/Embarked values\n",
        "train_df.dropna(axis=0, inplace=True)\n",
        "\n",
        "train_df.reset_index(inplace=True)\n",
        "\n",
        "# check shape of data after removing those rows/cols\n",
        "train_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoaneQrSQe5F"
      },
      "source": [
        "Similar to the home price data, let's see what the mutual information is for all of the predictors. We'll again have to convert categorial/string data to numerical values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-e1nbOcLQe5F"
      },
      "source": [
        "# Copy features and targets over to X, y, respectively\n",
        "X = train_df.copy()\n",
        "y = X.pop(\"Survived\")\n",
        "\n",
        "# Encoding categoricals\n",
        "for colname in X.select_dtypes('object').columns:\n",
        "    X[colname], _ = X[colname].factorize()\n",
        "\n",
        "# As before with the home prices, let's get mutual information plot results\n",
        "mi_scores = make_mi_scores(X, y)#, discrete_features)\n",
        "plt.figure(dpi=100, figsize=(5, 3))\n",
        "plot_mi_scores(mi_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCuFEAV4Qe5G"
      },
      "source": [
        "It's important to be careful with the mutual information results. Sometimes features that appear to have a relationship are not as useful as they seem because they themselves may have a relationship with a different feature, or may present some form of data leakage.\n",
        "\n",
        "For example, `index`, although neglible, appears to have a relationship with survival, which may indicate that the order of the observations in the dataset may be related to survival status in some way. It is not a feature that we would want to use for making predictions, however. `PassengerId` is similar. We can verify that there is no obvious relationship with a simple plot below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ncQkQjcQe5G"
      },
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8,3))\n",
        "ax1.scatter(X.index, y, alpha=0.2)\n",
        "ax1.set_title('Survival ~ index')\n",
        "ax2.scatter(X.PassengerId, y, alpha=0.2)\n",
        "ax2.set_title('Survival ~ PassengerId')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMiuGGGuQe5G"
      },
      "source": [
        "In addition to those two, we'll also drop `Ticket` and `Name` from our set of features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Shc2z7R7Qe5G"
      },
      "source": [
        "X.drop(columns=[\"index\", \"PassengerId\", \"Name\", \"Ticket\"], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jg8FTfqYQe5I"
      },
      "source": [
        "### 2. Modeling with RandomForest\n",
        "\n",
        "To model this data we will use the most common _go-to_ ML model in Data Science/ML, which is the Random Forest model. The advantages of an RF model is that it is relatively quick and easy to fit/train (as opposed to some other ML models, particularly NNs), but that it is still flexible/sophisticated enough to be able to capture complex non-linear relationships (as opposed to linear models).\n",
        "\n",
        "As you can imagine from the name, a RF is a collection of individual decision trees. We saw before that a decision tree model can easily overfit the data, whereas if we have a collection of independent decision trees, then we have another way to mitigate it (but overfitting is still possible!). RFs still often lead to high predictive power though, given that they are based on a collection of trees. In general, a collection of predictive models is known as an _ensemble_.\n",
        "\n",
        "We won't go into too much more detail with the inner workings of RFs, but if  you are curious they are well worth looking at. Although not the first paper on RFs, the [foundational paper by Breiman, L.](https://link.springer.com/article/10.1023%2FA%3A1010933404324) discusses the boostrapping approach used, and it's publication coincides with the rise of Data Science in the early 2000's."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NecS_JN2Qe5I"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf_model = RandomForestClassifier()\n",
        "rf_model.fit(X, y)\n",
        "y_pred = rf_model.predict_proba(X)\n",
        "\n",
        "# Plot the predictions side-by-side with the actual values\n",
        "# Note: a perfect model would yield the identity, y=x\n",
        "plt.scatter(y, y_pred[:,1], alpha=0.3)\n",
        "plt.xlabel('y actual')\n",
        "plt.ylabel('y predicted (probability)')\n",
        "plt.plot([0,1],[0.5,0.5], c='k', alpha=0.2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaEbSJDcQe5I"
      },
      "source": [
        "As can be seen above, this model fits fairly well. The figure above shows the binary outcomes (survival = 1) on the x-axis, and the predicted probability of surival on the y-axis. Note that the latter of these is not binary but is instead a probability, so a continuous value in $[0, 1]$.\n",
        "\n",
        "Note that for classification problems we ultimately want to have a binary prediction. However, as we can see in the plot above, our model outputs a predicted probability. It's up to us to determine what the threshold should be to make a positive or negative prediction. Oftentimes the default choice is to use a threshold of 0.5 (see the gray horizontal line above), but there may be cases when you'll want to choose another threshold."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwszhjtXQe5I"
      },
      "source": [
        "### 3. Loss functions\n",
        "\n",
        "Before we looked at the mean-squared error (MSE) as a loss function. The loss function, or sometimes more generally referred to as an objective function, is what is generally used to fit the model. That is, it is the function that we optimize to find the ideal values of the model's parameters or weights/biases. (Note that the terms _parameters_ and _weights_ can often have different meanings within the fields of Statistics vs Machine Learning.)\n",
        "\n",
        "MSE makes sense for regression problems when we have a continuous outcome. It can still be used in classification problems as well, since our models will generally predict a continuous probability between $[0, 1]$. So it's useful to know how the MSE is defined, which is as follows.\n",
        "\n",
        "__Mean-squared Error__\n",
        "\n",
        "$MSE(y, \\hat{y}) = \\frac{1}{n} \\sum_i (y_i - \\hat{y}_i)^2$\n",
        "\n",
        "When working with targets/outcomes that are categorical or discrete in nature there is another type of loss function that is commonly used, which is Cross-Entropy. Note that this can be used both for binary outcomes as well as multi-class outcomes.\n",
        "\n",
        "__Cross Entropy__\n",
        "\n",
        "$CE(y, \\hat{y}) = - \\sum_i y_i * \\mathrm{log}(\\hat{y}_i)$\n",
        "\n",
        "Let's calculate each of these now for our trained RF model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jn3KJuWaQe5J"
      },
      "source": [
        "# Calculate mean-squared error and binary cross entropy\n",
        "rf_mse = (((y - y_pred[:,1])**2).mean())\n",
        "print(\"mse: \", rf_mse)\n",
        "\n",
        "yhat = y_pred[:,1]\n",
        "\n",
        "# Calculate BCE manually\n",
        "def bce(y_obs, y_prd):\n",
        "    p0 = np.sum((1.00 - y_obs[y_obs == 0].to_numpy()) * np.log(1.00 - y_prd[y_obs == 0]))\n",
        "    p1 = np.sum(y_obs[y_obs == 1].to_numpy() * np.log(y_prd[y_obs == 1]))\n",
        "    return -1 * (p1 + p0)\n",
        "\n",
        "print(\"bce: \", bce(y, yhat))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDhwpmaIQe5J"
      },
      "source": [
        "### 4. Tuning hyperparameters\n",
        "\n",
        "As we now know, it's difficult to say whether this is a good value of the loss function. We have no other model to compare it to. Also, we have no validation/test set, so even if it the loss is low we don't know if there is overfitting going on.\n",
        "\n",
        "Let's create a train and test set as we did before.\n",
        "\n",
        "Furthermore, let's look at one of the hyperparameters for a RF model. As we said, RF models are often used because they generally don't require too much finetuning. But, there still are hyperparameters that can affect how well the model is able to generalize to new data.\n",
        "\n",
        "To accomplish this let's also right a function that will a) create the RF model, b) fit the model, and c) calculate the loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6U8cu6b4Qe5J"
      },
      "source": [
        "# Create train and test splits\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.80, random_state=1)\n",
        "\n",
        "# A function to create and fit a RF with a specific number of trees\n",
        "def fitRFModel(min_samples_split_hyper_param):\n",
        "    rf_model = RandomForestClassifier(min_samples_split=min_samples_split_hyper_param, random_state=1)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    y_train_pred = rf_model.predict_proba(X_train)[:,1]\n",
        "    y_val_pred = rf_model.predict_proba(X_val)[:,1]\n",
        "    train_loss = bce(y_train, y_train_pred) #F.binary_cross_entropy(torch.tensor(y_train_pred), torch.tensor(y_train.to_numpy().astype(float)), reduction=\"mean\")\n",
        "    val_loss = bce(y_val, y_val_pred) #F.binary_cross_entropy(torch.tensor(y_val_pred), torch.tensor(y_val.to_numpy().astype(float)), reduction=\"mean\")\n",
        "    return((train_loss.item(), val_loss.item()))\n",
        "\n",
        "# Possible values of min_samples_split are 10 to 70 (by 5)\n",
        "hyp_param_vals = list(range(10,71,5))\n",
        "losses = []\n",
        "for hp in hyp_param_vals:\n",
        "    losses.append(fitRFModel(hp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3IR-YSkQe5J"
      },
      "source": [
        "Let's now create a plot with the values of the min samples per leaf hyperparameter, `min_samples_leaf`, on the x-axis and the values of the loss function on the y-axis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVe17eCzQe5K"
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_axes([0, 0, 1, 1]) #.1, 0.1, 0.8, 0.8]) # main axes\n",
        "ax.plot(hyp_param_vals, [loss[1] for loss in losses], '--ro') # validattion loss\n",
        "ax.plot(hyp_param_vals, [loss[0] for loss in losses], '--bo') # training loss\n",
        "ax.legend([\"Validation Loss\", \"Train Loss\"])\n",
        "ax.set_xticks(hyp_param_vals)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxkkgLn2Qe5K"
      },
      "source": [
        "Starting on the left, at `min_samples_split=10`, we can conclude from the extremely low training loss and (relatively) high test/validation loss that  the model overfits there. Based on this plot it looks like the best value (that minimizes the validation loss) is around `min_samples_split=35`. You can verify this by commenting out the line above that plots the training loss so that only the test loss is shown.\n",
        "\n",
        "Note, however, that the validation loss does not really vary all that much. This is precisely the reason why Random Forests are so often the first model that Data Scientists and engineers will look to - because they often don't require much finetuning! That is to say, changing the hyperparameter values often has little impact on how well the model generalizes beyond the training data. You can see this by playing with other hyperparameter values such as, `n_estimators`, which sets how many decision trees are in the RF. Try changing the code above to explore how the loss varies when `n_estimators` changes (the default value for `n_estimators` is 100, so that is a good place to start).\n",
        "\n",
        "### 5. Training vs Validation vs Test\n",
        "\n",
        "We have used the terms _test_ and _validation_ interchangeably up until now. In practice there is a difference between them. The training dataset is, of course, used to train or fit the model itself. The validation and test datasets, however, are distinct subsets of the data that we hold out from the training. The validation dataset is used to help tune the hyperparameters, similar to as we did above. The test dataset is held out until the very end; that is, it is never used to make any decisions about how the model is trained or tuned. We imagine that the test dataset represents real-world data that our model has never before seen.\n",
        "\n",
        "A great example of a training, validation, and test dataset each being used is simply a Kaggle competition. Participants in a Kaggle competition have access to a single dataset, which they are individually responsible for splitting into a training and validation set in the way they deem best. Then, at the end of the competition, there is a test dataset that no participant had access to until that point. The test dataset is entirely new, and the participants models must make the best predictions possible using only the original training (and validation) dataset that they had access to earlier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktdGr6YNQe5K"
      },
      "source": [
        "### 6. Other Performance Metrics/Measures\n",
        "\n",
        "We have looked at loss functions as a way to measure the performance of our models, although their primary function is as the objective function in the optimization problem addressed when training/fitting a model. There are other performance metrics, particularly for classification, that are more intuitive for us to think about.\n",
        "\n",
        "The first of these is _accuracy_, which is simply the proportion of time that the model predicts the correct class. As an example, consider the same train and validation split above, and a RF model (with `min_samples_split=35`). The predicted probabilities for the validation dataset are shown below. Points shown in blue are correctly classified, while points shown in red are incorrectly classified.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8z9166WQe5K"
      },
      "source": [
        "rf_model = RandomForestClassifier(min_samples_split=35, random_state=1)\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_train_pred = rf_model.predict_proba(X_train)[:,1]\n",
        "y_val_pred = rf_model.predict_proba(X_val)[:,1]\n",
        "\n",
        "y_color = ['b' if (y_val_pred[i] > 0.5 and y_val.to_numpy()[i] == 1) or (y_val_pred[i] < 0.5 and y_val.to_numpy()[i] == 0) else 'r' for i in list(range(len(y_val)))]\n",
        "\n",
        "plt.scatter(y_val, y_val_pred, alpha=0.3, c=y_color, s=100)\n",
        "plt.xlabel('y actual')\n",
        "plt.ylabel('y predicted (probability)')\n",
        "plt.plot([0,1],[0.5,0.5], c='k', alpha=0.2)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1MiO81kQe5K"
      },
      "source": [
        "The same data depicted in the graph above is often communicated in a _confusion matrix_. For a binary classification problem the confusion matrix is simply a 2x2 matrix with the number of predicted vs actual outcomes. More generally, for a classification problem with $k$ classes, the confusion matrix will be $kxk$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K39SqdyJQe5L"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "y_pred = rf_model.predict(X_val)\n",
        "cm = confusion_matrix(y_val, y_pred)\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1])\n",
        "disp.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7Cf81BuQe5L"
      },
      "source": [
        "__Accuracy__\n",
        "\n",
        "Accuracy is straightforward to calculate from the confusion matrix - it is simply the ratio of correctly classified cases/observations to the total number of cases/observations.\n",
        "\n",
        "From the confusion matrix above we see that $76$ negative cases were correctly predicted to be negative (i.e. \"_true negatives_\"), while $34$ positive cases were correctly predicted to be positive (i.e. \"_true positives_\"), for a total of $110$ correctly predicted cases. The total number of cases in the validation dataset was $76 + 9 + 24 + 34 = 143$, yielding:\n",
        "\n",
        "* $\\mathrm{Accuracy} = 110 / 143 = 0.7692$\n",
        "\n",
        "__Precision and Recall__\n",
        "\n",
        "Accuracy works well in scenarios when the number of negative and positive cases are roughly balanced. However, when the proportion of negative to postive cases is skewed one way or the other, then accuracy is not the ideal performance measure. As an example, consider a scenario in which the proportion of positive cases in the data is very small (e.g. example could be trying to predict a rare disease, or flagging malicious content on a website, etc.). In scenarios such as this, then if the true proportion of positive cases is 0.01, then even a model that predicts every case to be negative will still have accuracy of 0.99.\n",
        "\n",
        "To overcome this deficiency of accuracy we can look to other performance metrics such as _precision_ and _recall_.\n",
        "\n",
        "Precision is the number of correctly predicted positive cases among all of the cases that were predicted as being positive.\n",
        "\n",
        "* $\\mathrm{precision} = \\frac{TP}{TP + FP}$\n",
        "\n",
        "Recall, also known as sensitivity, is the proportion of all observed positive cases that were predicted to be positive by the model.\n",
        "\n",
        "* $\\mathrm{recall} = \\frac{TP}{TP + FN}$\n",
        "\n",
        "The _F1 score_ is the combination precision and recall and is often the preferred performance metric for data with a very low proportion of positive (or negative) cases.\n",
        "\n",
        "* $\\mathrm{F}_1 = 2 * \\frac{precision * recall}{precision + recall}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWzMn_wvQe5L"
      },
      "source": [
        "### 7. Model interpretability\n",
        "\n",
        "We looked at the mutual information of each feature with the target variable earlier. However, the features that end up being most influential in a trained model can often vary from those results. This is because pre-modeling feature explorations, such as teh mutual information we looked at, are done pairwise and independent of the other features. It can sometimes be the case that a feature is more important when used immedi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCmKV6XyQe5L"
      },
      "source": [
        "rf_model = RandomForestClassifier(min_samples_split=35, random_state=1)\n",
        "rf_model.fit(X_train, y_train)\n",
        "importances = rf_model.feature_importances_\n",
        "std = np.std([tree.feature_importances_ for tree in rf_model.estimators_], axis=0)\n",
        "\n",
        "forest_importances = pd.Series(importances, index=X_train.columns)\n",
        "forest_importances.sort_values(ascending=True, inplace=True)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "forest_importances.plot.barh(xerr=std, ax=ax)\n",
        "ax.set_title(\"Feature importances using MDI\")\n",
        "ax.set_ylabel(\"Mean decrease in impurity\")\n",
        "fig.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztwjfbbXQe5M"
      },
      "source": [
        "## Further Reading\n",
        "\n",
        "If you are interested in learning more about any of the above material, the following short courses on Kaggle are another resource.  \n",
        "\n",
        "* Pandas - [https://www.kaggle.com/learn/pandas]()\n",
        "* Intro to ML - [https://www.kaggle.com/learn/intro-to-machine-learning]()\n",
        "* Feature Engineering - [https://www.kaggle.com/learn/feature-engineering]()\n",
        "* Intermediate ML - [https://www.kaggle.com/learn/intermediate-machine-learning]()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omH5b_koQe5M"
      },
      "source": []
    }
  ]
}